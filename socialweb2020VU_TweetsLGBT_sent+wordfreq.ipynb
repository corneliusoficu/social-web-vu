{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Social Web 2020 - VU course\n",
    "Part of the sentiment and wordfequency analysis and visualisation for 'A Sentiment Analysis of Tweets Regarding the LGBT Community in Different Countries'.\n",
    "\n",
    "Programmed by Sarah van Gerwen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.graph_objects as gojsn\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "import requests\n",
    "import json\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "from scipy.stats import chi2_contingency\n",
    "from scipy.stats import chi2\n",
    "from plotly.subplots import make_subplots\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ALL TWEETS: Sentiment visualization + chi-squared analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_sentiment(sentned, sentcan, sentsau, sentrus):\n",
    "    #takes sent files for ned, can, sau and rus with format: {\"positive\": positive, \"positive_proc\": positive_percentage, \"negative\": negative, \"negative_proc\": negative_percentage, \"neutral\": neutral, \"neutral_proc\": neutral_percentage, \"total\": total}\n",
    "    #open file\n",
    "    sentned = open_data(sentned)\n",
    "    sentcan = open_data(sentcan)\n",
    "    sentsau = open_data(sentsau)\n",
    "    sentrus = open_data(sentrus)\n",
    "    \n",
    "    #visualization\n",
    "    sentiment_visualisation(sentned, sentcan, sentsau, sentrus)\n",
    "    \n",
    "    #chi squared analysis x 4\n",
    "    df = pd.DataFrame.from_dict([sentned, sentcan, sentsau, sentrus]).T.rename(columns={0: 'The Netherlands', 1: 'Canada', 2: 'Saudi Arabia', 3: 'Russia'}).loc[['positive','negative','neutral', 'total']]\n",
    "    \n",
    "    #Experimen 1: all countries:\n",
    "    df_exp1 = df\n",
    "    print ('Sentiment chi-squared 1: Difference all countries')\n",
    "    chi2analysis(df_exp1)\n",
    "    \n",
    "    #Experiment 2: free vs non-free:\n",
    "    df_exp2 = df\n",
    "    df_exp2['free'] = df['The Netherlands'] + df['Canada']\n",
    "    df_exp2['nonfree'] = df['Saudi Arabia'] + df['Russia']\n",
    "    df_exp2 = df_exp2[['free', 'nonfree']]\n",
    "    print ('Sentiment chi-squared 2: Difference free vs non-free')\n",
    "    chi2analysis(df_exp2)\n",
    "    \n",
    "    #Experiment 3: all English:\n",
    "    df_exp3 = df[['The Netherlands', 'Canada', 'Saudi Arabia']]\n",
    "    print ('Sentiment chi-squared 3: Difference English')\n",
    "    chi2analysis(df_exp3)\n",
    "    \n",
    "    #Experiment 4: all native:\n",
    "    df_exp4 = df[['Canada', 'Russia']]\n",
    "    print ('Sentiment chi-squared 4: Difference native')\n",
    "    chi2analysis(df_exp4)\n",
    "\n",
    "def open_data(data):\n",
    "    #open file and return dictionary data\n",
    "    with open(data) as file:\n",
    "        dict_data = json.load(file)\n",
    "    return dict_data\n",
    "    \n",
    "def sentiment_visualisation(sentned, sentcan, sentsau, sentrus):\n",
    "    #prints pie charts and bar chart\n",
    "    \n",
    "    #pie chart\n",
    "    df = pd.DataFrame.from_dict([sentned, sentcan, sentsau, sentrus]).T.rename(columns={0: 'The Netherlands', 1: 'Canada', 2: 'Saudi Arabia', 3: 'Russia'}).loc[['positive','negative','neutral']]\n",
    "    pie_chart(df)\n",
    "    \n",
    "    #bar chart\n",
    "    df = pd.DataFrame.from_dict([sentned, sentcan, sentsau, sentrus]).rename(index={0: 'The Netherlands', 1: 'Canada', 2: 'Saudi Arabia', 3: 'Russia'}).loc[:,['positive_proc', 'negative_proc', 'neutral_proc']]\n",
    "    df = df.replace({'%':''}, regex = True)\n",
    "    df = df.apply(pd.to_numeric)\n",
    "    barplot(df)\n",
    "    \n",
    "def pie_chart(df):\n",
    "    labels = ['positive', 'negative', 'neutral']\n",
    "    country_list = ['The Netherlands', 'Canada', 'Saudi Arabia', 'Russia']\n",
    "    \n",
    "    for country in country_list:\n",
    "        if country == 'The Netherlands':\n",
    "            color_discrete_map = {'positive':'cornsilk','neutral':'orange','negative':'orangered'}\n",
    "        elif country == 'Canada':\n",
    "            color_discrete_map = {'positive':'lawngreen', 'neutral':'limegreen', 'negative':'darkgreen'}\n",
    "        elif country == 'Saudi Arabia':\n",
    "            color_discrete_map = {'positive':'lightcoral', 'neutral':'crimson', 'negative':'darkred'}\n",
    "        elif country == 'Russia':\n",
    "            color_discrete_map = {'positive':'lightcyan', 'neutral':'royalblue', 'negative':'darkblue'}\n",
    "    \n",
    "        piechrt = px.pie(df, values = country, names = labels, color = labels, color_discrete_map=color_discrete_map)\n",
    "        piechrt.update_traces(textposition='inside', textinfo='percent+label', showlegend=False)\n",
    "        piechrt.show()\n",
    "        \n",
    "def barplot(df):\n",
    "    barplot = df.plot.bar(color=['green', 'red', 'grey'])\n",
    "    fig = barplot.get_figure()\n",
    "    fig.savefig('bar_sent.png')\n",
    "\n",
    "def chi2analysis(df):\n",
    "    #get the chi squared result with scipy\n",
    "    table = [[df.loc['positive'].tolist()], [df.loc['negative'].tolist()], [df.loc['neutral'].tolist()]]\n",
    "    chi, p, dof, ex = chi2_contingency(table)\n",
    "    \n",
    "    print ('chisquared= ' + str(chi))\n",
    "    print ('crit= ' + str(chi2.ppf(0.95, dof)))\n",
    "    print ('p= ' + str(p))\n",
    "    print('dof= ' + str(dof))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_sentiment('tweets-netherlands-2020-03-14_19_59_49_sentiments.json', 'tweets-canada-2020-03-14_19_36_20_sentiments.json',\n",
    "               'tweets-saudiarabia-2020-03-14_20_54_54_sentiments.json', 'tweets-russia_sentiments.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ALL TWEETS: Word frequency analysis + sent analysis with VADER + visualization + chi-squared analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_frequency_analysis(rawbatchned, rawbatchcan, rawbatchsau, rawbatchrus):\n",
    "    #open file (Format json {\"id\": id, \"id_str\": id_str, \"text\": text, etc}) \n",
    "    #do sentiment analysis with VADER for ned can and sau\n",
    "    dfned = open_file_sent(rawbatchned)\n",
    "    dfcan = open_file_sent(rawbatchcan)\n",
    "    dfsau = open_file_sent(rawbatchsau)\n",
    "\n",
    "    #open file, translate russian tweets, apply VADER ## in this case, only the first 82 Tweets were selected\n",
    "    dfrus = pd.read_json(rawbatchrus, lines=True, encoding='utf8')\n",
    "    dfrus = translation(dfrus)\n",
    "    dfrus = dfrus.iloc[:82]\n",
    "    dfrus = VADER_sentiment(dfrus)\n",
    "    \n",
    "    #name dfs to keep track\n",
    "    dfnedname = 'ned'\n",
    "    dfcanname = 'can'\n",
    "    dfsauname = 'sau'\n",
    "    dfrusname = 'rus'\n",
    "    \n",
    "    #get frequencies of words and make wordclouds\n",
    "    dfned = get_frequencies(dfned, dfnedname)\n",
    "    dfcan = get_frequencies(dfcan, dfcanname)\n",
    "    dfsau = get_frequencies(dfsau, dfsauname)\n",
    "    dfrus = get_frequencies(dfrus, dfrusname)\n",
    "    \n",
    "    #do the different chi-squared experiments\n",
    "    between_comparison_wordfreq(dfrus, dfsau, dfcan, dfned, dfrusname, dfsauname, dfcanname, dfnedname)\n",
    "\n",
    "def open_file_sent(data):\n",
    "    #adds sentiment score, returns new df\n",
    "    #open file with format json {\"id\": id, \"id_str\": id_str, \"text\": text, etc}\n",
    "    df = pd.read_json(data, lines=True, encoding='utf8')\n",
    "    #sentiment analysis\n",
    "    df = VADER_sentiment(df)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def VADER_sentiment(df):\n",
    "    #use VADER to get sentiment scores -> https://github.com/cjhutto/vaderSentiment for thresholds and more info\n",
    "    #return dataframe with all information + column with sentiment\n",
    "    sentiment = []\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    \n",
    "    for tweet in df['text']:\n",
    "        VADER_score = sid.polarity_scores(tweet)\n",
    "        for key,value in VADER_score.items():\n",
    "            if key == 'compound':\n",
    "                if value >= 0.05:\n",
    "                    sentiment.append('positive')\n",
    "                if value <= -0.05:\n",
    "                    sentiment.append('negative')\n",
    "                if value > -0.05 and value < 0.05:\n",
    "                    sentiment.append('neutral')\n",
    "                    \n",
    "    df['sentiment'] = sentiment\n",
    "    return (df)\n",
    "    \n",
    "def translation(df):\n",
    "    #adapted from https://github.com/cjhutto/vaderSentiment/blob/master/README.rst#demo-including-example-of-non-english-text-translations\n",
    "    #return df with translated Tweets instead of original Tweets\n",
    "    \n",
    "    sentences_translated = []\n",
    "    \n",
    "    for tweet in df['text']:\n",
    "        api_url = \"http://mymemory.translated.net/api/get?q={}&langpair={}|{}\".format(tweet, 'ru', 'en')\n",
    "        hdrs ={'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.11 (KHTML, like Gecko) Chrome/23.0.1271.64 Safari/537.11',\n",
    "                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
    "                'Accept-Charset': 'ISO-8859-1,utf-8;q=0.7,*;q=0.3',\n",
    "                'Accept-Encoding': 'none',\n",
    "                'Accept-Language': 'en-US,en;q=0.8',\n",
    "                'Connection': 'keep-alive'}\n",
    "        response = requests.get(api_url, headers=hdrs)\n",
    "        response_json = json.loads(response.text)\n",
    "        translation = response_json[\"responseData\"][\"translatedText\"]\n",
    "        translator_name = \"MemoryNet Translation Service\"\n",
    "        sentences_translated.append(translation)\n",
    "        \n",
    "    df['text'] = sentences_translated\n",
    "    return df\n",
    "\n",
    "def get_frequencies(df, dfname):\n",
    "    #takes df and tokenizes + tagges a tokenized tweet, checks whether tokens are in labelled tweet, preprocesses the resulting data\n",
    "    #returns df with labelled words\n",
    "    sent_dict = dict()\n",
    "    \n",
    "    for i in range(df.shape[0]):\n",
    "        #tokenize tweet and tag the words with part-of-speech tag\n",
    "        tokenized_tweet = nltk.word_tokenize(df['text'].iloc[i])\n",
    "        tagged_tweets = nltk.pos_tag(tokenized_tweet)\n",
    "        sent = df['sentiment'].iloc[i]\n",
    "        for word, tag in tagged_tweets:\n",
    "            #only take nouns\n",
    "            if tag == 'NN':\n",
    "                if word not in sent_dict:\n",
    "                    sent_dict[word] = {'positive' : 0, 'negative' : 0, 'neutral' : 0, 'total' : 0}\n",
    "                sent_dict[word][sent] += 1\n",
    "                sent_dict[word]['total'] += 1\n",
    "    \n",
    "    #preprocess data\n",
    "    sent_dict = wordfreq_preprocessing(sent_dict)\n",
    "    \n",
    "    #add overall label\n",
    "    df = pd.DataFrame(sent_dict).transpose()\n",
    "    df['label_context'] = df[['positive','negative', 'neutral']].idxmax(axis=1)\n",
    "        \n",
    "    #prints five most frequent entries for each label\n",
    "    printedlist = ['positive', 'negative', 'neutral', 'total']\n",
    "    for label in printedlist:\n",
    "        df = df.sort_values(by=[label], ascending = False)\n",
    "        print ('Five most frequent %s words from %s' % (label, dfname))\n",
    "        print (df.head(5))\n",
    "    \n",
    "    #make wordcloud\n",
    "    wordcloud(df, dfname, 'total')\n",
    "    \n",
    "    return (df)\n",
    "    \n",
    "def wordfreq_preprocessing(freq):\n",
    "    #takes frequencies in dictionary form, removes interpunction/words/upper-lower difference\n",
    "    #returns frequencies in dict\n",
    "    #remove punctuation and selected words\n",
    "    no = [\"n't\", 'I', '’', \"'s\", '...', '``', \"''\", '“', '”', \"'m\", 'Is', 'In', 'It', 'Are', 'https', 'The', 'a', 'And', 'The', 'A', 'Of', 't', 'For', 'If', 'By', 'An','So', 'To', 'However', 'Whence', 'Whenever', 'Where', 'Whereby', 'Whereever', 'Wherein', 'Whereof', 'That', 'What', 'Whatever', 'Which', 'Whichever']\n",
    "    for entry in no:\n",
    "        if entry in freq:\n",
    "            del freq[entry]\n",
    "        elif entry.lower() in freq:\n",
    "            del freq[entry.lower()]\n",
    "        elif entry.upper() in freq:\n",
    "            del freq[entry.upper()]\n",
    "    for punc in string.punctuation:\n",
    "        if punc in freq:\n",
    "            del freq[punc]\n",
    "    \n",
    "    #remove lower-uppercase difference\n",
    "    list_of_dup = []\n",
    "    for word in freq:\n",
    "        if word in freq and word.lower() in freq:\n",
    "            list_of_dup.append(word)\n",
    "    for dup in list_of_dup:\n",
    "        if dup == dup.upper():\n",
    "            for scores in freq[dup]:\n",
    "                freq[dup][scores] = freq[dup][scores] + freq[dup.lower()][scores]\n",
    "            del freq[dup.lower()]\n",
    "        elif dup != dup.upper() and dup != dup.lower():\n",
    "            try:\n",
    "                for scores in freq[dup]:\n",
    "                    freq[dup.lower()][scores] = freq[dup][scores] + freq[dup.lower()][scores]\n",
    "                del freq[dup]\n",
    "            except:\n",
    "                pass\n",
    "    return freq\n",
    "                \n",
    "            \n",
    "def wordcloud(df, whichmask, whichone):\n",
    "    #check https://github.com/amueller/word_cloud for details\n",
    "    #takes df, whichmask : ('can','rus', 'ned', 'sau', 'free', 'nonfree', 'comb'), whichone : columns\n",
    "    #take masked images in different colours and outputs a wordcloud\n",
    "    d = df[whichone].to_dict()\n",
    "    \n",
    "    if whichmask == 'can':\n",
    "        maskedimage = 'cangreen.png'\n",
    "        out = str(whichone) + str(whichmask) + '_wordcloud.png'\n",
    "    elif whichmask == 'ned':\n",
    "        maskedimage = 'nedorange.png'\n",
    "        out = str(whichone) + str(whichmask) + '_wordcloud.png'\n",
    "    elif whichmask == 'sau':\n",
    "        maskedimage = 'saured.png'\n",
    "        out = str(whichone) + str(whichmask) + '_wordcloud.png'\n",
    "    elif whichmask == 'rus':\n",
    "        maskedimage = 'rusblue.png'\n",
    "        out = str(whichone) + str(whichmask) + '_wordcloud.png'\n",
    "    elif whichmask == 'comb':\n",
    "        maskedimage = 'togetherpurp.png'\n",
    "        out = str(whichone) + str(whichmask) + '_wordcloud.png'\n",
    "    elif 'nonfree' in whichmask:\n",
    "        maskedimage = 'nonfreedarkred.png'\n",
    "        out = str(whichone) + str(whichmask) + '_wordcloud.png'\n",
    "    else:\n",
    "        maskedimage = 'freedarkgreen.png'\n",
    "        out = str(whichone) + str(whichmask) + '_wordcloud.png'\n",
    "    \n",
    "    \n",
    "    mask = np.array(Image.open(maskedimage))\n",
    "    image_colors = ImageColorGenerator(mask)\n",
    "    wordcloud = WordCloud(background_color=\"white\", mask = mask)\n",
    "    wordcloud.generate_from_frequencies(frequencies=d)\n",
    "    wordcloud.recolor(color_func=image_colors).to_file(out)\n",
    "    plt.figure()\n",
    "    plt.imshow(wordcloud.recolor(color_func=image_colors), interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "    \n",
    "def between_comparison_wordfreq(rus,sau,can,ned,namerus,namesau,namecan,namened):\n",
    "    #EXPERIMENT 1: OUTER OUTER INNER\n",
    "    #experiment 1.1: all differences - positive - negative - neutral - total\n",
    "    print ('\\n EXPERIMENT 1 Intersection \\n')\n",
    "    print ('\\n 1.1 All differences \\n')\n",
    "    df = config(rus,sau,can,ned,True,False,False,False)\n",
    "    labels_and_chisquared(df,True,False,False,False)\n",
    "    \n",
    "    #experiment 1.2: free vs nonfree - positive - negative - neutral - total\n",
    "    print ('\\n 1.2 Free non-free \\n')\n",
    "    df = config(rus,sau,can,ned,True,False,False,True)\n",
    "    labels_and_chisquared(df,False,False,False,True)\n",
    "    \n",
    "    #experiment 1.3: English - positive - negative - neutral - total\n",
    "    print ('\\n 1.3 English \\n')\n",
    "    df = config(rus,sau,can,ned,True,True,False,False)\n",
    "    labels_and_chisquared(df,False,True,False,False)\n",
    "    \n",
    "    #experiment 1.4: Native - positive - negative - neutral - total\n",
    "    print ('\\n 1.4 Native \\n')\n",
    "    df = config(rus,sau,can,ned,True,False,True,False)\n",
    "    labels_and_chisquared(df,False,False,True,False)\n",
    "    \n",
    "    #EXPERIMENT 2: OUTER OUTER OUTER\n",
    "    #experiment 2.1: all differences - positive - negative - neutral - total\n",
    "    print ('\\n EXPERIMENT 2 Union \\n')\n",
    "    print ('\\n 2.1 All differences \\n')\n",
    "    df = config(rus,sau,can,ned,False,False,False,False)\n",
    "    labels_and_chisquared(df,True,False,False,False)\n",
    "    \n",
    "    #experiment 2.2: free vs nonfree - positive - negative - neutral - total\n",
    "    print ('\\n 2.2 Free non-free \\n')\n",
    "    df = config(rus,sau,can,ned,False,False,False,True)\n",
    "    labels_and_chisquared(df,False,False,False,True)\n",
    "    \n",
    "    #experiment 2.3: English - positive - negative - neutral - total\n",
    "    print ('\\n 2.3 English \\n')\n",
    "    df = config(rus,sau,can,ned,False,True,False,False)\n",
    "    labels_and_chisquared(df,False,True,False,False)\n",
    "    \n",
    "    #experiment 2.4: Native - positive - negative - neutral - total\n",
    "    print ('\\n 2.4 Native \\n')\n",
    "    df = config(rus,sau,can,ned,False,False,True,False)\n",
    "    labels_and_chisquared(df,False,False,True,False)\n",
    "    \n",
    "    #EXPERIMENT 3: TOPIC MODELLING\n",
    "    print ('\\n EXPERIMENT 3 Topics of free vs non-free \\n')\n",
    "    print ('\\n 3.3 Combined words \\n')\n",
    "    freenonfree_freq_clouds(ned,namened,can,namecan,'outer','freeouter')\n",
    "    freenonfree_freq_clouds(sau,namesau,rus,namerus,'outer','nonfreeouter')\n",
    "    df = config(rus,sau,can,ned,False,False,False,False)\n",
    "    df['togetherouter'] = df['total_ned'] + df['total_can'] + df['total_rus'] + df['total_sau']\n",
    "    wordcloud(df, 'comb', 'togetherouter')\n",
    "    \n",
    "    print ('\\n 3.3 Combined intersected words \\n')\n",
    "    freenonfree_freq_clouds(ned,namened,can,namecan,'inner','freeinner')\n",
    "    freenonfree_freq_clouds(sau,namesau,rus,namerus,'inner','nonfreeinner')\n",
    "    df = config(rus,sau,can,ned,True,False,False,False)\n",
    "    df['togetherinner'] = df['total_ned'] + df['total_can'] + df['total_rus'] + df['total_sau']\n",
    "    wordcloud(df, 'comb', 'togetherinner')\n",
    "    \n",
    "    print ('\\n 3.3 Unique words \\n')\n",
    "    df = config(rus,sau,can,ned,False,False,False,False)\n",
    "    unique_words(df)\n",
    "    \n",
    "def config(rus,sau,can,ned,inner,english,native,groups):\n",
    "    #take all dfs, inner: True/False, english: True/False, native: True/False, groups: True/False configure new dataframes\n",
    "    #return new df\n",
    "    \n",
    "    if english == False and native == False and groups == False:\n",
    "        if inner == True:\n",
    "            df_merges1 = ned.merge(can, how='inner', left_index = True, right_index = True, suffixes=('_ned', '_can'))\n",
    "            df_merges2 = rus.merge(sau, how='inner', left_index = True, right_index = True, suffixes=('_rus', '_sau'))\n",
    "            df = df_merges1.merge(df_merges2, how='inner', left_index = True, right_index = True, suffixes=('', '')).fillna(0)\n",
    "        else:\n",
    "            df_merges1 = ned.merge(can, how='outer', left_index = True, right_index = True, suffixes=('_ned', '_can'))\n",
    "            df_merges2 = rus.merge(sau, how='outer', left_index = True, right_index = True, suffixes=('_rus', '_sau'))\n",
    "            df = df_merges1.merge(df_merges2, how='outer', left_index = True, right_index = True, suffixes=('', '')).fillna(0)\n",
    "\n",
    "    elif groups == True:\n",
    "        if inner == True:\n",
    "            df_merges1 = ned.merge(can, how='outer', left_index = True, right_index = True, suffixes=('_ned', '_can'))\n",
    "            df_merges2 = rus.merge(sau, how='outer', left_index = True, right_index = True, suffixes=('_rus', '_sau'))\n",
    "            df = df_merges1.merge(df_merges2, how='inner', left_index = True, right_index = True, suffixes=('', '')).fillna(0)\n",
    "       \n",
    "        else:\n",
    "            df_merges1 = ned.merge(can, how='outer', left_index = True, right_index = True, suffixes=('_ned', '_can'))\n",
    "            df_merges2 = rus.merge(sau, how='outer', left_index = True, right_index = True, suffixes=('_rus', '_sau'))\n",
    "            df = df_merges1.merge(df_merges2, how='outer', left_index = True, right_index = True, suffixes=('', '')).fillna(0)\n",
    "       \n",
    "    elif english == True:\n",
    "        if inner == True:\n",
    "            df_merges1 = ned.merge(can, how='inner', left_index = True, right_index = True, suffixes=('_ned', '_can'))\n",
    "            df = df_merges1.merge(sau, how='inner', left_index = True, right_index = True, suffixes=('', '')).fillna(0)\n",
    "        else:\n",
    "            df_merges1 = ned.merge(can, how='outer', left_index = True, right_index = True, suffixes=('_ned', '_can'))\n",
    "            df = df_merges1.merge(sau, how='outer', left_index = True, right_index = True, suffixes=('', '')).fillna(0)\n",
    "    \n",
    "    elif native == True:\n",
    "        if inner == True:\n",
    "            df = can.merge(rus, how='inner', left_index = True, right_index = True, suffixes=('_can', '_rus')).fillna(0)\n",
    "        else:\n",
    "            df = can.merge(rus, how='outer', left_index = True, right_index = True, suffixes=('_can', '_rus')).fillna(0)\n",
    "         \n",
    "    return df\n",
    "    \n",
    "def labels_and_chisquared(df,entire,english,native,groups):\n",
    "    #take configurated df, inner: True/False, english: True/False, native: True/False, groups: True/False configure new dataframes\n",
    "    #creates new dfs based on labels and performs the chisquared analysis\n",
    "    labels = ['positive', 'negative', 'neutral', 'total']\n",
    "    for label in labels:\n",
    "        if entire == True:\n",
    "            for word,value in df.iterrows():\n",
    "                if value[(label + '_ned')] == 0 and value[(label + '_can')] == 0 and value[(label + '_rus')] == 0 and value[(label + '_sau')] == 0:\n",
    "                    df = df.drop(index=word)\n",
    "            table = [ [df[(label + '_ned')].tolist()], [df[(label + '_can')].tolist()], [df[(label + '_rus')].tolist()], [df[(label + '_sau')].tolist()]]\n",
    "            print ('\\n Chi squared results for label %s \\n' % (label))\n",
    "            chi2analysis(table)    \n",
    "        elif groups == True:\n",
    "            df['free'] = df[(label + '_ned')]+df[(label + '_can')]\n",
    "            df['nonfree'] = df[(label + '_rus')]+df[(label + '_sau')]\n",
    "            for word,value in df.iterrows():\n",
    "                if value[(label + '_ned')] == 0 and value[(label + '_can')] == 0 and value[(label + '_rus')] == 0 and value[(label + '_sau')] == 0:\n",
    "                    df = df.drop(index=word)\n",
    "            table = [ [df['free'].tolist()], [df['nonfree'].tolist()]]\n",
    "            print ('\\n Chi squared results for label %s \\n' % (label))\n",
    "            chi2analysis(table)\n",
    "        elif english == True:\n",
    "            for word,value in df.iterrows():\n",
    "                if value[(label + '_ned')] == 0 and value[(label + '_can')] == 0 and value[label] == 0:\n",
    "                    df = df.drop(index=word)\n",
    "            table = [ [df[(label + '_ned')].tolist()], [df[(label + '_can')].tolist()], [df[label].tolist()] ]\n",
    "            print ('\\n Chi squared results for label %s \\n' % (label))\n",
    "            chi2analysis(table)\n",
    "        elif native == True:\n",
    "            for word,value in df.iterrows():\n",
    "                if value[(label + '_can')] == 0 and value[(label + '_rus')] == 0:\n",
    "                    df = df.drop(index=word)\n",
    "            table = [ [df[(label + '_can')].tolist()], [df[(label + '_rus')].tolist()] ]\n",
    "            print ('\\n Chi squared results for label %s \\n' % (label))\n",
    "            chi2analysis(table)\n",
    "                \n",
    "def chi2analysis(table):\n",
    "    chi, p, dof, ex = chi2_contingency(table)\n",
    "    \n",
    "    print ('chisquared= ' + str(chi))\n",
    "    print ('crit= ' + str(chi2.ppf(0.95, dof)))\n",
    "    print ('p= ' + str(p))\n",
    "    print('dof= ' + str(dof))\n",
    "\n",
    "def freenonfree_freq_clouds(country1,name1,country2,name2,how,columnname):\n",
    "    df = country1.merge(country2, how=how, left_index = True, right_index = True, suffixes=(('_'+name1), ('_'+name2))).fillna(0)\n",
    "    df[columnname] = df[('total_'+name1)] + df[('total_'+name2)] \n",
    "    \n",
    "    df = df.sort_values(by=[columnname], ascending = False)\n",
    "    print ('\\n Most frequent 25 words for %s \\n' % (columnname))\n",
    "    print (df.head(25))\n",
    "    \n",
    "    wordcloud(df, columnname, columnname)\n",
    "\n",
    "def unique_words(df):\n",
    "    #takes df, generates unique words and makes wordclouds out of them\n",
    "    df['free'] = df['total_ned'] + df['total_can']\n",
    "    df['nonfree'] =  df['total_rus'] + df['total_sau']\n",
    "    df_free = df\n",
    "    df_nonfree = df \n",
    "    \n",
    "    for word,value in df.iterrows():\n",
    "        if value['free'] != 0 or '/' in word:\n",
    "            df_nonfree = df_nonfree.drop(index=word)\n",
    "        if value['nonfree'] != 0 or '/' in word:\n",
    "            df_free = df_free.drop(index=word)\n",
    "    \n",
    "    df_free = df_free.sort_values(by=['free'], ascending = False)\n",
    "    df_nonfree = df_nonfree.sort_values(by=['nonfree'], ascending = False)\n",
    "    \n",
    "    print ('\\n Most frequent 25 unique words for the free countries \\n')\n",
    "    print (df_free.head(25))\n",
    "    wordcloud(df_free, 'free', 'free')\n",
    "    print ('\\n Most frequent 25 unique words for the non-free countries \\n')\n",
    "    print (df_nonfree.head(25))\n",
    "    wordcloud(df_nonfree, 'nonfree', 'nonfree')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_frequency_analysis('tweets-netherlands-2020-03-14_19_59_49', 'tweets-canada-2020-03-14_19_36_20', \n",
    "                        'tweets-saudiarabia-2020-03-14_20_54_54', 'tweets-russia-2020-03-14_16_37_32')"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
